---
title: "Milestone Report "
author: "Shadi"
date: "6/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

The goal of this project is to show the mastrey in dealing with the data, and that data scientist  is on track to create precise  prediction algorithm for text models like those used by SwiftKey.
Text data is collected from publicly available sources by a web crawler, and it is available in coursera.
The data consist of 3 text files containing text from three different sources (blogs, news & twitter), and it is availbale in 4 languages. we focus our analysis on english data set.

## Exploratory Analysis 

we start by loading data and library 

### Load Library and data

```{r load libray and data}

library(magrittr)
library(stringi)
library(ggplot2)
library(tm)
library(RWeka)



blogs<-readLines("en_US.blogs.txt",encoding="UTF-8", skipNul = TRUE)
news<-readLines("en_US.news.txt",encoding="UTF-8", skipNul = TRUE)
twitter<-readLines("en_US.twitter.txt",encoding="UTF-8", skipNul = TRUE)

```



## Exploratory analysis

We continue our analysis by providing summary stataistic and histogram for of blog, twitter and news


### Statistics

Summary statistic of raw data is provided below.
   

```{r , echo=TRUE}
words_blogs <- stri_count_words(blogs)
words_news <- stri_count_words(news)
words_twitter <- stri_count_words(twitter)
size_blogs <- file.info("en_US.blogs.txt")$size/1024^2
size_news <- file.info("en_US.news.txt")$size/1024^2
size_twitter <- file.info("en_US.twitter.txt")$size/1024^2
summary_table <- data.frame(filename = c("blogs","news","twitter"),
                            file_size_MB = c(size_blogs, size_news, size_twitter),
                            num_lines = c(length(blogs),length(news),length(twitter)),
                            num_words = c(sum(words_blogs),sum(words_news),sum(words_twitter)),
                            mean_num_words = c(mean(words_blogs),mean(words_news),mean(words_twitter)))
summary_table
```
looking  at initial statistics we see that our data size is so big; therefore to prevent difficulty in calcutlation  we take a sample of our data set with 1% size of it is original size


```{r , echo=TRUE}

set.seed(1)
blogsSample <- sample(blogs, length(blogs)*0.01)
newsSample <- sample(news, length(news)*0.01)
twitterSample <- sample(twitter, length(twitter)*0.01)

```

### Plots

Before preparing our data for analysis, we take a look at the histogram of data per line for each file.

```{r , echo=TRUE}

wpl <- lapply(list(blogs, news, twitter), function(x) stri_count_words(x))


plot1 <- qplot(wpl[[1]],
               geom = "histogram",
               main = "US Blogs",
               xlab = "Words per Line",
               ylab = "Frequency",
               binwidth = 5,
               colour = I("green"))

plot2 <- qplot(wpl[[2]],
               geom = "histogram",
               main = "US News",
               xlab = "Words per Line",
               ylab = "Frequency",
               colour = I("Yellow") ,
               binwidth = 5)

plot3 <- qplot(wpl[[3]],
               geom = "histogram",
               main = "US Twitter",
               xlab = "Words per Line",
               ylab = "Frequency",
               binwidth = 1,
               colour = I("blue"))

plot1
plot2
plot3
rm(plot1, plot2, plot3)

```
### Data preparation for prediction

After taking a sample of data set we combine the three samples. 

```{r , echo=TRUE}
sample.corpus  <- c(blogsSample,newsSample,twitterSample)
corpus <- Corpus(VectorSource(list(sample.corpus)))

```
The basic procedure for data preprocessing consists of the following key steps:

1. Construct a corpus from the files.

2. Tokenization. Clean up the corpus by removing special characters, punctuation,   numbers etc. We also remove profanity that we do not want to predict.

3. Build basic n-gram model.

We will require the following helper functions in order to prepare our corpus.

```{r , echo=TRUE}

toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
preprocessCorpus <- function(corpus){
    # Helper function to preprocess corpus
    corpus <- tm_map(corpus, toSpace, "/|@|\\|")
    corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removeNumbers)
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeWords, stopwords("english"))
    corpus <- tm_map(corpus, stripWhitespace)
    return(corpus)
}

freq_frame <- function(tdm){
    # Helper function to tabulate frequency
    freq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
    freq_frame <- data.frame(word=names(freq), freq=freq)
    return(freq_frame)
}
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
QuadgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=4, max=4))


```
We preprocess the text sample to form Term Document Matrices.

```{r , echo=TRUE}
text_sample <- VCorpus(VectorSource(corpus))
text_sample <- preprocessCorpus(text_sample)

tdm1a <- TermDocumentMatrix(text_sample)
tdm1 <- removeSparseTerms(tdm1a, 0.99)
freq1_frame <- freq_frame(tdm1)

tdm2a <- TermDocumentMatrix(text_sample, control=list(tokenize=BigramTokenizer))
tdm2 <- removeSparseTerms(tdm2a, 0.999)
freq2_frame <- freq_frame(tdm2)

tdm3a <- TermDocumentMatrix(text_sample, control=list(tokenize=TrigramTokenizer))
tdm3 <- removeSparseTerms(tdm3a, 0.9999)
freq3_frame <- freq_frame(tdm3)

tdm4a <- TermDocumentMatrix(text_sample, control=list(tokenize=QuadgramTokenizer))
tdm4 <- removeSparseTerms(tdm4a, 0.9999)
freq4_frame <- freq_frame(tdm4)
```

##  Analysis
For each Term Document Matrix, we list the most common unigrams, bigrams, trigrams and fourgrams.

```{r , echo=TRUE}

ggplot(freq1_frame[1:20,], aes(x=reorder(word,freq), y=freq, fill=freq)) +
    geom_bar(stat="identity") +
    theme_bw() +
    coord_flip() +
    theme(axis.title.y = element_blank()) +
    labs(y="Frequency", title="Most common unigrams in text sample")

```


```{r , echo=TRUE}

ggplot(freq2_frame[1:20,], aes(x=reorder(word,freq), y=freq, fill=freq)) +
    geom_bar(stat="identity") +
    theme_bw() +
    coord_flip() +
    theme(axis.title.y = element_blank()) +
    labs(y="Frequency", title="Most common bigrams in text sample")
```


```{r , echo=TRUE}

ggplot(freq2_frame[1:20,], aes(x=reorder(word,freq), y=freq, fill=freq)) +
    geom_bar(stat="identity") +
    theme_bw() +
    coord_flip() +
    theme(axis.title.y = element_blank()) +
    labs(y="Frequency", title="Most common trigrams in text sample")
```
## Future Plan

 As a next step a model will be created and integrated into a Shiny app for word prediction.

We will use the Ngram dataframes created to calculate the probability of the next word occuring. The input string will be tokenized and the last 2 (or 1 if itâ€™s a unigram) words will be isolated and cross checked against the data frames to get the highest probability next word.

The model will then be integrated into a shiny application that will provide a simple and intuitive front end for the ned user.

